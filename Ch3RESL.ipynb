{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the F statistic (3.13) for dropping a single coefficient\n",
    "from a model is equal to the square of the corresponding z-score (3.12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_1-p_0=1$, $RSS = (Y-X\\beta)^T(Y-X\\beta)$, \n",
    "\n",
    "$\\frac{(RSS_0-RSS_1)}{RSS_1/(N-p_1-1)} = \\beta_j^2/(\\sigma^2(X^TX)_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data on two variables X and Y , consider fitting a cubic\n",
    "polynomial regression model $f(X) =\n",
    "\\sum^3_{j=0} \\beta_jX_j$. In addition to plotting\n",
    "the fitted curve, you would like a 95% confidence band about the curve.\n",
    "Consider the following two approaches:\n",
    "1. At each point x_0, form a 95% confidence interval for the linear function\n",
    "$a^T\\beta =\\sum^3_{j=0} \\beta_jx_j$\n",
    "\n",
    "2. Form a 95% confidence set for β as in (3.15), which in turn generates\n",
    "confidence intervals for f(x0).\n",
    "\n",
    "How do these approaches differ? Which band is likely to be wider? Conduct\n",
    "a small simulation experiment to compare the two methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach consider percentile for normal distribution for each $\\beta$. The second approach we consider percentile for chi distribution for set of $\\beta$s. I would expect second band to be wider all beta contribute at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>X</th><th scope=col>Y</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1.99676405</td><td>12.5180369 </td></tr>\n",
       "\t<tr><td> 0.14887276</td><td> 1.1051888 </td></tr>\n",
       "\t<tr><td> 0.02975812</td><td>-0.9762164 </td></tr>\n",
       "\t<tr><td>-0.10232201</td><td>-2.0374831 </td></tr>\n",
       "\t<tr><td> 1.30950677</td><td> 5.1682533 </td></tr>\n",
       "\t<tr><td>-1.64164290</td><td>-4.0566385 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " X & Y\\\\\n",
       "\\hline\n",
       "\t  1.99676405 & 12.5180369 \\\\\n",
       "\t  0.14887276 &  1.1051888 \\\\\n",
       "\t  0.02975812 & -0.9762164 \\\\\n",
       "\t -0.10232201 & -2.0374831 \\\\\n",
       "\t  1.30950677 &  5.1682533 \\\\\n",
       "\t -1.64164290 & -4.0566385 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| X | Y |\n",
       "|---|---|\n",
       "|  1.99676405 | 12.5180369  |\n",
       "|  0.14887276 |  1.1051888  |\n",
       "|  0.02975812 | -0.9762164  |\n",
       "| -0.10232201 | -2.0374831  |\n",
       "|  1.30950677 |  5.1682533  |\n",
       "| -1.64164290 | -4.0566385  |\n",
       "\n"
      ],
      "text/plain": [
       "  X           Y         \n",
       "1  1.99676405 12.5180369\n",
       "2  0.14887276  1.1051888\n",
       "3  0.02975812 -0.9762164\n",
       "4 -0.10232201 -2.0374831\n",
       "5  1.30950677  5.1682533\n",
       "6 -1.64164290 -4.0566385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=rnorm(20)\n",
    "Y = 2*X+X^2+0.6*X^3+rnorm(20)\n",
    "df = data.frame(X, Y)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>-1.1396592 </td><td>-0.01312149</td></tr>\n",
       "\t<tr><th scope=row>X</th><td> 0.9334383 </td><td> 3.42163117</td></tr>\n",
       "\t<tr><th scope=row>I(X^2)</th><td> 0.5351249 </td><td> 1.51890494</td></tr>\n",
       "\t<tr><th scope=row>I(X^3)</th><td> 0.1749300 </td><td> 0.97235379</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & 2.5 \\% & 97.5 \\%\\\\\n",
       "\\hline\n",
       "\t(Intercept) & -1.1396592  & -0.01312149\\\\\n",
       "\tX &  0.9334383  &  3.42163117\\\\\n",
       "\tI(X\\textasciicircum{}2) &  0.5351249  &  1.51890494\\\\\n",
       "\tI(X\\textasciicircum{}3) &  0.1749300  &  0.97235379\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 2.5 % | 97.5 % |\n",
       "|---|---|---|\n",
       "| (Intercept) | -1.1396592  | -0.01312149 |\n",
       "| X |  0.9334383  |  3.42163117 |\n",
       "| I(X^2) |  0.5351249  |  1.51890494 |\n",
       "| I(X^3) |  0.1749300  |  0.97235379 |\n",
       "\n"
      ],
      "text/plain": [
       "            2.5 %      97.5 %     \n",
       "(Intercept) -1.1396592 -0.01312149\n",
       "X            0.9334383  3.42163117\n",
       "I(X^2)       0.5351249  1.51890494\n",
       "I(X^3)       0.1749300  0.97235379"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = lm(Y~X+I(X^2)+I(X^3), data=df)\n",
    "confint(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ X + I(X^2) + I(X^3), data = df)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-1.6461 -0.4081 -0.1458  0.4246  2.1160 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  -0.5764     0.2657  -2.169 0.045470 *  \n",
       "X             2.1775     0.5869   3.710 0.001900 ** \n",
       "I(X^2)        1.0270     0.2320   4.426 0.000424 ***\n",
       "I(X^3)        0.5736     0.1881   3.050 0.007639 ** \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.9701 on 16 degrees of freedom\n",
       "Multiple R-squared:  0.978,\tAdjusted R-squared:  0.9739 \n",
       "F-statistic: 237.1 on 3 and 16 DF,  p-value: 1.812e-13\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td> 1.8172327</td><td> 2.736903 </td></tr>\n",
       "\t<tr><th scope=row>poly(X, 3)1</th><td>22.5039022</td><td>26.616793 </td></tr>\n",
       "\t<tr><th scope=row>poly(X, 3)2</th><td> 5.5220314</td><td> 9.634923 </td></tr>\n",
       "\t<tr><th scope=row>poly(X, 3)3</th><td> 0.9022406</td><td> 5.015132 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & 2.5 \\% & 97.5 \\%\\\\\n",
       "\\hline\n",
       "\t(Intercept) &  1.8172327 &  2.736903 \\\\\n",
       "\tpoly(X, 3)1 & 22.5039022 & 26.616793 \\\\\n",
       "\tpoly(X, 3)2 &  5.5220314 &  9.634923 \\\\\n",
       "\tpoly(X, 3)3 &  0.9022406 &  5.015132 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 2.5 % | 97.5 % |\n",
       "|---|---|---|\n",
       "| (Intercept) |  1.8172327 |  2.736903  |\n",
       "| poly(X, 3)1 | 22.5039022 | 26.616793  |\n",
       "| poly(X, 3)2 |  5.5220314 |  9.634923  |\n",
       "| poly(X, 3)3 |  0.9022406 |  5.015132  |\n",
       "\n"
      ],
      "text/plain": [
       "            2.5 %      97.5 %   \n",
       "(Intercept)  1.8172327  2.736903\n",
       "poly(X, 3)1 22.5039022 26.616793\n",
       "poly(X, 3)2  5.5220314  9.634923\n",
       "poly(X, 3)3  0.9022406  5.015132"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = lm(Y~poly(X,3), data=df)\n",
    "confint(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ poly(X, 3), data = df)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-1.6461 -0.4081 -0.1458  0.4246  2.1160 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)   2.2771     0.2169  10.498 1.39e-08 ***\n",
       "poly(X, 3)1  24.5603     0.9701  25.318 2.46e-14 ***\n",
       "poly(X, 3)2   7.5785     0.9701   7.812 7.53e-07 ***\n",
       "poly(X, 3)3   2.9587     0.9701   3.050  0.00764 ** \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.9701 on 16 degrees of freedom\n",
       "Multiple R-squared:  0.978,\tAdjusted R-squared:  0.9739 \n",
       "F-statistic: 237.1 on 3 and 16 DF,  p-value: 1.812e-13\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least if to consider poly function the confidence intervals are wider for the same error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gauss–Markov theorem:\n",
    "(a) Prove the Gauss–Markov theorem: the least squares estimate of a\n",
    "parameter $a^T$β has variance no bigger than that of any other linear\n",
    "unbiased estimate of $a^T$β (Section 3.2.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let take estimater which differs from least square estimator by bector $b^T$. So that\n",
    "\n",
    "$E(c^T Y) = E(a^T\\beta+b^TY) = E(a^T\\beta) + E(b^tX\\beta)$\n",
    "\n",
    "because we have unbias estimator \n",
    "\n",
    "$=a^T\\beta$ Eq.(3.18)\n",
    "\n",
    "$b^TX = 0$, taking this into account to remove cross products\n",
    "\n",
    "$(c^Ty) = c^TVar(y)c = \\sigma^2 c^Tc = Var(a^T\\beta - b^TY)(a^T \\beta - b^TY)^T = \\sigma^2 (a^Ta/(X^TX)+b^Tb) > Var(a^T\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) The matrix inequality B =< A holds if A − B is positive semidefinite.\n",
    "Show that if $\\hat{V}$ is the variance-covariance matrix of the least squares\n",
    "estimate of β and $V$ is the variance-covariance matrix of any other\n",
    "linear unbiased estimate, then $\\hat{V} =< V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive semidefinite $X^T(A-B)X >= 0$ for any real X. $\\hat{V} = E(a^T\\beta a\\beta^T)-E(a^T\\beta)E(a\\beta^T)$, $V = E(c^T\\beta c\\beta^T)-E(c^T\\beta)E(c\\beta^T)$ from part (a) = $\\hat{V}+E(b^Tb)>=\\hat{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how the vector of least squares coefficients can be obtained\n",
    "from a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Represent\n",
    "your solution in terms of the QR decomposition of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition $\\beta=X^TY/(XX^{T})$, $x_j =z_j + \\sum_{k=1}^{j-1} \\gamma_{jk}z_k $ or $X=Z\\Gamma$\n",
    "\n",
    "QR dicomposition: $X=QR$, where Q is orthogonal matrix, R is upper triangular matrix, Z=QD, $\\Gamma=D^{-1}R$, D is diagonal matrix.\n",
    "\n",
    "$\\beta = Q^TR^TY/(QRQ^{T}R^{T})=Q^TY/R$\n",
    "\n",
    "Using that R is upper triangular matrix (see also wiki QR decomposition)\n",
    "$r_{pp}\\beta_{p} = <q_{p}, y>$\n",
    "\n",
    "$\\beta_{p} = <z_p, y>/||z_{p}||^2$\n",
    "\n",
    "Substituting back for equation for $\\beta_j$,\n",
    "\n",
    "$r_{p-1, p-1}\\beta_{p-1} +r_{p-1, p}\\beta_p=<q_p, y>$  \n",
    "\n",
    "$\\beta_{p-1} +\\gamma_{p-1,p}\\beta_p=<z_{p-1}, y>/||z_{p-1}||^2$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the ridge regression problem (3.41). Show that this problem\n",
    "is equivalent to the problem\n",
    "\n",
    "$\\hat{\\beta^c}= argmin_{\\beta_c}{(\\sum_{i=1}^{N}(y_i-\\beta_0^c-\\sum_{j=1}^p(x_{ij}-\\bar{x}_j)\\beta^c_j)^2+\\lambda\\sum_{j=1}^p\\beta_j^{c2})}$\n",
    "\n",
    "Give the correspondence between βc and the original β in (3.41). Characterize\n",
    "the solution to this modified criterion. Show that a similar result\n",
    "holds for the lasso.\n",
    "\n",
    "\n",
    "We can write $\\hat{\\beta^c}= argmin_{\\beta_c}{(\\sum_{i=1}^{N}(y_i-(\\beta_0^c-\\sum_{j=1}^p\\bar{x}_{j})-\\sum_{j=1}^px_{ij}\\beta^c_j)^2+\\lambda\\sum_{j=1}^p\\beta_j^{c2})}$\n",
    "\n",
    "$\\beta_0 = \\beta_0^c-\\sum_{j=1}^p\\bar{x}_{j}$, $\\beta_j=\\beta_j^c$ $j>0$\n",
    "\n",
    "It does not depend on regulization term if it does not contain $\\beta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the ridge regression estimate is the mean (and mode)\n",
    "of the posterior distribution, under a Gaussian prior $\\beta ~ N(0, \\tau I)$, and\n",
    "Gaussian sampling model $y ~ N(X\\beta, \\sigma^2I)$. Find the relationship between\n",
    "the regularization parameter $\\lambda$ in the ridge formula, and the variances $\\tau$\n",
    "and $\\sigma^2$.\n",
    "\n",
    "$\\hat{\\beta}_R = X^Ty(X^TX+\\lambda I)^{-1}$\n",
    "\n",
    "From Gaussian distribution:\n",
    "\n",
    "$\\hat{\\beta}_R = argmax_{\\beta}(L(y|\\beta)\\pi(\\beta))$, where L is Gaussian likelyhood, $\\pi(\\beta)$ is probability, $p(\\beta|y)=p(y|\\beta)\\pi(\\beta)$.\n",
    "Taking log from argument\n",
    "\n",
    "$\\hat{\\beta}_R = argmax_{\\beta}(-(y-X\\beta)^T(y-X\\beta)/\\sigma^2-\\beta^T\\beta/\\tau^2)=argmin_{\\beta}((\\beta^TX^TX\\beta-\\beta^TX^Ty-y^TX\\beta)/\\sigma^2+\\beta^T\\beta/\\tau^2)$ \n",
    "\n",
    "If gradient by $\\beta$ in argmin is 0 we have\n",
    "\n",
    "$\\hat{\\beta}_R = X^Ty(X^TX + \\sigma^2/\\tau^2  I)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This problem is similar or even part of problem 6 where $log(L(y|\\beta)\\pi(\\beta))$ is written by components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the QR decomposition of the uncentered N × (p + 1)\n",
    "matrix X (whose first column is all ones), and the SVD of the N × p\n",
    "centered matrix $\\hat{X}$. Show that Q2 and U span the same subspace, where\n",
    "Q2 is the sub-matrix of Q with the first column removed. Under what\n",
    "circumstances will they be the same, up to sign flips?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q is orthoganal matrix the columns of Q are $q_1, .., q_{p+1}$.  By QR decomposition $span(q_i,... q_j) = span(x_i, ..,x_j), 1<=j<=p+1$ or for Q2 if we shift indexes by -1  $span(q_i,... q_j) = span(x_i, ..,x_j), 1<=j<=p$ (1)\n",
    "\n",
    "We can write $\\hat{x}_i = \\sum_{j<i} a_jq_j$, so $span(q_1, .., q_i)=span(x_1,.., x_i)$ (2)\n",
    "For SVD $span(u_1,.., u_p) = span(\\hat{x}_1,.., \\hat{x}_p)$ (3). Therefore from (1) -(3) $span(q_i,... q_j) = span(u_1,.., u_p)$, Q2 and U span the same subspace.\n",
    "\n",
    "If $sign(\\hat{x_i})=sign(x_i)$(X is without original first column) for any $i$ they must be the same up to sign flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward stepwise regression. Suppose we have the QR decomposition\n",
    "for the N×q matrix X1 in a multiple regression problem with response\n",
    "y, and we have an additional p−q predictors in the matrix X2. Denote the\n",
    "current residual by r. We wish to establish which one of these additional\n",
    "variables will reduce the residual-sum-of squares the most when included\n",
    "with those in X1. Describe an efficient procedure for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add $X_{21}$ to matrix $X_{1}$, do QR decomposition, find $\\beta$ and new residual r. Compare it to initial r if r get smaller save $X_{21}$ and corresponding sum. Try the same on next, compare with the current saved smallest r. Do it for all predictors in $X_2$. This way find which predictor has smallest r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward stepwise regression. Suppose we have the multiple regression\n",
    "fit of y on X, along with the standard errors and Z-scores as in\n",
    "Table 3.2. We wish to establish which variable, when dropped, will increase\n",
    "the residual sum-of-squares the least. How would you do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider most of p-1 models removing 1 feature. Choose model with smallest  residual sum-of-squares. I would start from highest absolute value of z score, but not rely too much on it. If the residual sum-of-squares really started to increase more for smaller Z score and Z scores distribution does not change much in new models, then it may be Ok not to check features with smallest  scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the solution to the multivariate linear regression problem (3.40) is given by (3.39). What happens if the covariance matrices $\\sum_i$\n",
    "are different for each observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have the same $\\epsilon_k$ for all observations it does not participate in $\\sum_i$, minimizing $RSS(B, \\sum)$ we come\n",
    "again to equation $X^T\\sum^{-1}(Y-X\\hat{B}) = 0$ in case if $\\sum$ is independent from $i$, $\\hat{B}=X^TY/(X^TX)$. If $\\sum_ik$\n",
    "are deferent for different i, we have like different weights in equation and we must leave $\\sum$ in it  $\\hat{B}=X^T\\sum^{-1}Y/(X^TX)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the ridge regression estimates can be obtained by\n",
    "ordinary least squares regression on an augmented data set. We augment\n",
    "the centered matrix X with p additional rows $\\sqrt{\\lambda}I$, and augment y with p\n",
    "zeros. By introducing artificial data having response value zero, the fitting\n",
    "procedure is forced to shrink the coefficients toward zero. This is related to\n",
    "the idea of hints due to Abu-Mostafa (1995), where model constraints are\n",
    "implemented by adding artificial data examples that satisfy them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function is $\\sum_{j=1}^{N+p}\\sum_{k=0}^p(y_{j}-x_{jk}\\beta_k)^2=\\sum_{j=1}^{N}\\sum_{k=0}^p(y_{j}-x_{jk}\\beta_k)^2+\\sum_{j=N+1}^{N+p}\\lambda(\\sum_{k=0}^p I_{jk}\\beta_k)^2 =\\sum_{j=1}^{N}\\sum_{k=0}^p(y_{j}-x_{jk}\\beta_k)^2+\\sum_{k=0}^p\\lambda\\beta_k^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the expression (3.62), and show that $\\hat{\\beta}_{pcr}(p) = \\hat{\\beta}_{ls}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_m = Xv_m$, substituting in (3.61) we get $\\hat{y}_{pcr} =\\bar{y}{\\bar{1}}+ X\\sum_{m=1}^M\\hat{\\theta}_mv_m$, $\\hat{\\beta}_{pcr} = \\sum_{m=1}^M\\hat{\\theta}_mv_m = \\hat{\\beta}_{ls}$, because $\\hat{y}_{ls}=\\bar{y}{\\bar{1}}+X\\hat{\\beta}_{ls}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that in the orthogonal case, PLS stops after m = 1 steps,\n",
    "because subsequent $\\hat{\\phi}_{mj}$ in step 2 in Algorithm 3.3 are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $x_j^{(m)}$, equation in part 2(d). In orthogonal case in part 2 (d) of Algorithm 3.3 we obtain that in $<z_m, x_j^{(m-1)}>=\\hat{\\phi}_{mj}x_j^{(m-1)2}$, m=2, we have only this component. Then $<x_j^{(m)},y> = <x_j^{(m-1)},y>-(\\hat{\\phi}_{mj}x_j^{(m-1)2}<z_m, y>)/\\sum_{j=1}^p{\\hat{\\phi}_{mj}^2x_j^{(m-1)2}}=\\hat{\\phi}_{mj}-\\hat{\\phi}_{mj}=0$, if m>1, $z_m = \\sum_{j=1}^p \\phi_{mj}x^{(m-1)}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify expression (3.64), and hence show that the partial least\n",
    "squares directions are a compromise between the ordinary regression coefficient\n",
    "and the principal component directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 3.3 is maximization of covariance matrix $cov_{||\\alpha=1||}(Y, \\alpha X)^2 = corr(Y, \\alpha X)^2Var(Y) Var(\\alpha X)$\n",
    "\n",
    "$cov_{||\\alpha=1||}(Y, \\alpha X)^2 = \\alpha^TX^TYY^TX\\alpha$\n",
    "\n",
    "Condition of maximum eigen value and eigen vector pair of this symmetic matrix $X^TYY^TX$  is 3.64\n",
    "\n",
    "In Algorithm 3.3 $\\phi_{mj}$ are products $<x_{j}, y>$ at the end we come to 3.64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
