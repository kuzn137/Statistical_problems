{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charpter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 2.1 Suppose each of K-classes has an associated target tk, which is a\n",
    "vector of all zeros, except a one in the kth position. Show that classifying to\n",
    "the largest element of $\\hat{y}$ amounts to choosing the closest target, $min_k||t_k − \\hat{y}||$, if the elements of ˆy sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y_k=max(y)$ $T(y)=t_k$. Using Eq.(2.23) with $G_k => t_k$, $x=>\\hat{y}$, $||t_k - \\hat{y}||^2=y_1^2+..+(1-y_k)^2+..+y_K^2$ \n",
    "\n",
    "Because $max(\\hat{y})<1$, $max(Pr(T|Y=\\hat{y})) = min_k||t_k-\\hat{y}||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decission boundary is where classes probabilities are equil. In considered case the vounndary is Pr(y=1|X=x)=Pr(y=0|X=x)=0.5\n",
    "It is not clear what exact distribution in the book. Below there is simple case of two 2D normal distributions with different mean. Boundary in 2D plane is pdf1(x1,x2) = pdf2(x1,2)) (1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y=rep(0, 200)\n",
    "y[101:200]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = function(x1, x2, mu1, mu2){\n",
    "    return(1/sqrt(2*pi)*exp(-(x1-mu1)^2/2-(x2-mu2)^2/2))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity for first and second distribution for x1 means $\\mu_{11}=\\mu_{21}$\n",
    "In this case solution of Eq. (1) $x2 = (\\mu_{12} + \\mu_{22})/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAAP8AzQBNTU1o\naGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD/AAD///9SdC1QAAAACXBI\nWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO3diVbjOhZGYTchQFF1geT9H7YhExk8aPh1dCTv\nb/Wq4laTOCHasSWHZNgDyDbUvgFADwgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRA\ngJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQ\nAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAAB\nQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUIC\nBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQI\nCRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQ\nICRAgJAAAUICBAxCGoDGJIxyfTgVNgEoERIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBA\nSIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIxna7Xe2bgAIIydShIlLqECGZ2l39iZ4QkqXd\n3d/oBiFZIqRuEZIlQuoWIZlijtQrQjLFql2vCMkY55H6REiAACEBAoTUGA4NfSKkprBY4RUh\nNYXlc68IqSWc0HWLkFpCSG4RUksIyS1CagpzJK8IqSms2nlFSI3hPJJPhAQIEBIgQEiAACEB\nAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEFIHeEV4fYTUPH5HyQNCah6/NesBIbWO\n93FwgZBaR0guEFLrCMkFQmoecyQPCKl5rNp5QEgd4DxSfYQECBASIEBIgAAhAQKEBAgQEiBA\nSIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiA\nACEBAoQECBASIEBIPeG9i6shpH7wbvoVEVI/+HyXigipG3ziWE2E1A1pSMy2IhFSN4QhMduK\nRkj90M2RmG1FI6R+yPYjzLbiEVJPRDMbQopHSHhASPEICY+YI0UjJDxi1S4aIWEM55EiERIg\nQEiAACEBAoQECBASIEBIgAAhAQKElITTLLhFSAk48Y97hJSAl6LhHiHFK/HiaI4VG0dI8Y4H\ndjthSBwrNo+Q4u3kA59jxeYRUoJjQpKQDod0/CJd+wgpwXmHlD3wd+ejxNN/5l7fxEYItDxC\nSrA7Dc78kI5/lgyJ6ZcNQkqgGviX6yk4R2L6ZYOQUqSMzpEjrN3N/1Vkt8H0ywghpYgf+KOX\nuBrlpSYyhGSEkNLEDvzxfVj54y5CMkJIJibGs8FKAHMkG4RU2vyZouJr0zGtslCejpDKMjlT\ntHQTQjPal949RmsnbUIq63xk1cARlr+b6DHtKYRUlMECt4rDZQl/aU8jpKIMFrhV/IXk7xbN\nIKSiGhoL/m6qv1s0g5DKaujoZCf+JatshJSto5Dcz40u/N3Uhp6FCKm4enOjyC272yM5THua\nZUhfr8Ow/Xe6ktlr6SmkWmJHocsDKe8rNL8MQ/raDD9ejldCSIXFHhe5DKkdhiG9De/fNb1v\ntocrIaSyorsgpCyGIW2OF/zcPH8SUnHxXbQ0tffHMKRzO1/b7VhIw7XETeAiIaSGpvb+GIb0\nPHydv9qyRyouYQfTztTeH8OQ3ofX01efw5aQSmMHY8py+fvtUs+/haM3QlJgB2PI9ITsx8v5\nq89XQkJPeGUDIEBIgAAhAQKEBAgQEiBASGChXICQ0NapW6fRExJaerWq2+gJafWa+v0Jt9ET\n0uq1FJLf20pIq+d3cD7ye1sJCW4Plx4RkrtN4MLtBH6E2+gJCW6XlEe4jZ6QkrUz+rri9MdO\nSIncPjVecTrmukRIiSoerAf2YZM6sR4RUpp6y0fBfVik3sJ+2QYhLRt70q0YUuB2TW7h1I1Z\n336KkJaMP+lWC+n2MwBDvtHixtz98wr3U4S0ZOJJt9Yc6bTdw0elz269ZkjFN+wPIS2YGo+1\nnnXPg3S3XxqqBsN5/Ifj9+UHBRHSgulRUWkecCwooBKL1EdvBiEVu4jDTYRyNyp2R8evF7+1\n+I3ZP8bq7kdmYfUhBU7a3QyK3eGD9U43x8Gtml7SdHDjDK08pIDDH2dLULuHP/1x9iMzsfaQ\nrv6c/ibjMTG3vfOuyPlQ5TxSmYs43MSBx6P5+UauTiN5utEgpOu/PZjfSXq8xdgT0s3fHp7l\nl0oJfoWQg/uyJusO6Xbq7mLesRhS0K2U3BdajLD2kK4GnI+VsOVjt5DxLbgvPp5XmrHykK6G\npZfZh6JnwX05/Vxq/zSasfqQLtyEJNgTZN+X3xfF1v5xtIKQzryEpJib5Id0eVFs/R9HGwjp\nosoc6eqFc9Krvfoz7eK767+xiJAuKsyuzy+ck28187787o7oKBAhXTFf791d/U991Tn35Zwh\nC+DBCKkiv0dQx5tFRuEIqSLHIXESKRIh6UQ/g/sNid1RLEJSSXkSLzhHgi1CUklZcS62ane6\ndvo0Q0giiedAC51H2jPNMUZIIn5eGHHi4zW4q0FIIt5C8nZ7ekdIKs72AIRki5BUnM1JCMkW\nIen4WiWrvof09eMojZB6VWIPGdGGsx10cYTUL/UuIaqN6jtEY4SEUDFtrG6KRkgIFNVGekiN\nTq0IybeFYWU56kxCanZqRUieLQwr21EX10biHKnZqRUhebYwrFIHa2J7UZtLi3wyVvcHfITk\n2MI+IO3wKX03FnnJlLE/cZcaOOAjJMfKhJRwmctlSw/mqZDG/tEXQnKsREjO16VHk3F+mw8I\nybMCc6TAQVlrTtLsh9ISkmepq3YzGQQNyppzkpHbTkipCOks5TxSyGf+zY9Jb3MSb7dnBCH1\n52HY3eQW8vnTd39Xx6pdIkLKcJ/BwyhcnP9IQ9LMtjiPlISQMjyEdPNfKdeQc2P870s0CKk5\nS0/OdxmkVKF7yUQDsxsNQmpM8BRnd/zuXVJIKfuRsQu5m20VQ0g1ZBzx75Y/R+8yom+GduwL\nieL3RyObISTtRRxuoqKMecNvIwvfdtwf7e//LGi0GULSXsThJirKGNmXQEIufd4V2cz3x5th\njiS9iMNN1JPxLL27DMyIkPal3hR5Ymu3/8qqnfIiDjdRT1ZI5zWEwhtKcbvG8fvPa8iIkCrI\nDCniKd72uGp8jWMtCMlezhxpf3yKD3uaLzeix7c/ssaxGoRkL3/VLvgKyhxXhbwq9jCRW9Fu\niZBqyDyPVPspf377v2sc+xUd4RFSeyImWUX2CQvbv/2/CUl5EYebaFhwSIX2CUvb3wV9V2cI\nqT3hIYV9m3r7t/0SkvAiDjfRssBAig3lxe3/TuQISXoRh5toWeAhW9xQln9kS8DLaztCSEWV\nWgCeeK+Gsde5Bb4uL246tXy/dqfTXYSkvIjDTVgwXQAe2VjEHEk+nfo94bUOhFSQ6QLwyMbC\n365LPqFZ2QyJkEoyHUzjGwt9uy5CykVI5eQPppgFgPCNje27wi8tuTn9HfQRUjm5ozNqihW+\nsdHvDDsKjQ57/Nt7fPEQIRWUOUeKu3jwd4+HFDC248Ie/+anp6c+XzxESAXlPfNG7tCCNzZx\nvQEr2jE3Z/QKfyo6/hF1TS0gpKJy5gLRR4ahG0vcIVxuT/p9err6k5AIqbjd8Y2CCr02IHFH\neerv8juw8U4F7c5/J12JU4Tkz/m398qd00y61vOOLD3up5u/u+qIkBy6PPU7W9w6FpSxUnAJ\nydkdUyAkd04D7HgIFfbGWzbOt+jwddI1XOZInEdKu4jDTfh1FdLVf7qQ9k7iv25X7bpCSO44\nDmmffQ7oqXBG1XZ1hOTPeY509R9+eJu43ah44wjJn9tVu9q35oHj+U3F5x5C8uj8Tot+h6xL\nNV9yTkjoBiFV2ATSeN5LElKFTSDF2O8EOiqLOZL9JpDiYaT6WhFh1c5+E0jweOzkbY2e80jW\nm0CCh5Bqzkp8ISSEI6RJhIQI90dyhHRGSIjwMJv3NkeqhpCc87S6/GN3v/i9d7RqVxEhudbA\nOPVWeiWE5BpHTq0gJM+YyzeDkDwjpGYQkmchIUnnKEx4UhGSa4tzJOlqRANLG24RkrG45/zF\noS1djWBpIx0hmYp/zp8PTzqJYkaWgZBMqZ/zCckLQrIkH6qE5AUhWZobqmkLZsyRnCAkS9Mh\npS6Ytb9q18mKOyGZmnzOT98ZtH0eqZsVd0IydTNurt6+1/zMqxfdHE0SkrHdb0b73zeUXw6p\nm6fuG/2sbxBSLTcfAxkQ0sL/3yZCKqvXkK6Ozm4/vW6xk35G3I1+7hYhFTE6n7mdIO1v/l46\ncutnxN3qZkdLSAVMVHEzaO5CMn0tkCPdTP0IqYDxp9m7Fm7mSKnX2YFOFiMJSW9i73EfUtzH\nQLb31N1JIYEISS8spOiPgWxrYLYXfh5C0puaz0y/rKHDAdftoegEQipgYhBNrUH0+Nzd6+LI\nJEIqYDKN8VXxqz89i9pvElKRizjcRFkRg66RIRe532zkXukQUm2NDLnY/WYr+1kVQqqtjZCi\nb+V5D9bjQsoY05D++/My/Hh5+6/UJhrUxHN3Qu67Q0b7/hZSRhmG9PU8/NoW2USTQgdb1ef2\nxP1mE08SEoYhvQ2bvx+Hrz7/bYa3EptoVEgitZ/bk5Jo47BVwjCkzfBx+fpj2JTYRMfSBrKu\nvKSQCUl8kePlhqn/OP3LlcRN9CtlSIp3YglVEpL4IgfskTIkhRR9CTkHN8GI7Rzp3+fhK+ZI\n0VJWzeIvIld7ZmfHcvl7e3Xs9vxVZBNtmzt4in9u9xBSny/IHWN7HuntcB5p8/KH80iP5p+9\nk95+//Dt6xjItfHKBjeW9jmRz+2Hb/85J5p3qxCGkLxQH4kdX1awliOr6gjJC3FIx6M6OrJC\nSF6UCEl3dVhASIVEviHDXn3OhZBsEVIRkW8RdJB+zqXpX7ztBCEVEfmmdSeJnzW2oreC8IuQ\nSnh4G9WSJnc9rDQYIqQSLENiMuQCIcl97wgIaXUISew4NUmbI6Vt8O5vVEFIYsfxnLJql7dF\nOqqLkLQu+4f480jJm2R5zgFC0qpyoNXq8pzdk015hKTFjCWY5eFveTkhfb0Ow/bf6R+lY7/d\nkJixBDNckDGQEdLX5vhuj8d/JKSjKjOWFo/tTE9al5cR0tvw/l3T++bwXo+EdDE9qguN9zZX\nGwjpfJHN8YvPzfMnIQUoNt7jjia9zPAJ6XyRcztf2y0hBSg1e4pa33A0w2eOdPr7eTi/E9Dz\nlpAWFVvPiwvp6s/KHDUtkBHS+/B6+upz2BLSEhch+Tqe8nKUqZCz/P12qeef+F2GCSn+miN2\nSF5C6knWCdmPl/NXn6+EtCR3jjS55hexikFIpfDKBjNh433qcGf20uHr6o7mSH0hJEPL4316\nAi5a8+trhu9IZkiv54W7z/mP4MvYxFocKpvcYehmWD3N8B3JDGnY/D38/c5iQ57r3wccK4kX\nwzqXGdJ/m+Hl83t3NGwW3hY/fRN9ut8vnH4f8Pz/Pnz/bEgtvtSuN9lzpD/D8DYMf0Q3Z3QT\n/XmYqZxCeDr//XiR6TlSmy+1603+YsP3Ud3Pq1elug/p6s+D3dT/8fsdk7nwixseiPZIs5+/\nl7uJ7jwewZ0y2M0tqk0cwDF7ciF/jrT9niO9WM2R/gfUFD9kw0b5+aju74ZVu3Ajawq/n2UU\nu2dhj+RCZkjbz9MXX69j35qq85BGp0KXI7e0kuioLl7ZUMPoVCh118KqnQeEVMfI6wvSj9E4\nj1QfIfnBMVrDCMkPjtEaRkiecIzWLEK6xWujkYSQrvHbOkhESNf4/VEkIqQrvKMBUhHSFUJC\nKkK6QkhIRUjXmCMhESFdY9UOiQjpFueRkISQAAFCAgQICRBYbUhdTYa6ujNtWmlIkctzvl+V\nzVqjA2sN6erPRd5/T4izXw6sM6S4lzA4/81VXo/hASEt8v5BEITkASEtUoVUaipDSB6sM6So\naYUspIht+rhihFtrSDF7B80cqdyOg1U7B1YaUtR8RbNqV/IILGny5XtNvzWrDSmKYsw5m8p4\nX9NvDSGZ8TWVcb6m3xxCMuNqKsNnWIgRkiFHL4kjJDFCWidCEiOklWKOpEVIK8WqnRYhrRbn\nkZQICfccrYm0g5Bwy9UqfTsICbd8nTduBiHhhrNXMjWDkHCDkNIQUqd+Vwzi1g4IKQ0hdel3\nxSB67YA5UhJC6tJvDdFdsGqXhJB69Ht8lnKk1ux5pJo3nJCSFX3Y8q48L6RG1d2VElKiog9b\n7pWvM6SrP+0RUqKiD1v2lWfMkVpV+SmDkNIUfdjyrzxj1a5VhFRnE5m8hDQ5l0o9j9QsQqqz\niUw+QlrN7iYEc6Qqm8jlYo5kO3ac79pYtauyiVweVu3Cd4uCBsR3uESVnEeqsIl8dueRprYU\nGlJsA6Pbk+79+jsmJST3pgddcEhB3zW/Pe2ksL9FeUKqK2C3NjPowsbjRAPz+7miIXV4mpiQ\nago5wpkbdGFHSKPXMHnRqexmbkY0Qkq8iMNNuBCyR5kfdCETtfGQpq50anvKozFCSryIw014\nEDSeFr9puaWRBqavdTIk5foAc6S0izjchAdhT8zzgy7o6PDxe2Y2Pb2vUi5+P9yixhFSRYEh\nzQ66wPWG+8vPhWQyyJ2f3Y1GSDUFHuHMDLrk2cbcpi0HeS9BEVIywVv+5j/5p4fk4uDKx60I\nNlM9ISUSvQl97hNyxvqXh31BU4sOs9UTUiIvH4uSOhRd7AvaWgaf/VETUho3H9SVGoSLfUFT\nIc3fWEJaMHEA5CakxEO0gCFs8LEvhFSYm5Amn++dhBT0IvHRC979/SBlDhiftIv9YiBCyjD9\nQHuYI91kHneMtxzS1Z8Jt6bgZaphjpRsZrh5+OjIm0c25Mn9apex8O0Je9y0vYuHtcNArNol\nm33erv7RkTe3bvSm3o7SmB1YfEhNzXcScR4pUe7gKPt0uxTSfSt3u4zZG0dIkQhpVtZkuPQE\nYDGk23+IG+nRcyRCMriIw02EyWqh+JLU/BzpfmRHhnQ9Bwzas7a0AqdHSAvSj87KP0XPT3ry\nQrqaA4a+p1FLK3ByhFSMxbHO/Xmkp4cp0cOxXfzNCb5cQytwcoRUjP2k4W6fcB9A4i5j5ZOf\nQIRUjvmk4WFZbn8XTqlXE4GQCrKeNDyOeMmxFiGFIKSSIgby47dGV1BqxK97OS4QIbkwdhR2\n/y/LV3L3t8q6l+MCEZILj0/6KbuBYruONS/HBSKk+q4+E+z33x7+JeiK9uw6KiGk2n4G/jml\nzJDYddRDSLUd9iKP53zu/p6/CvKpjpAqOxf0tE+dI1U7oKPfK4RU2SmY4wHe9b8H51FpdZoJ\n2Q1CquxyDJd6HqnW+VLOLt0gpNpyB2SlkHi9wy1Cqi33EImQXCCk+k7HcKlz9zrHWIR0i5Cc\nSN8xVZr1M0e6QUhO5IzLKuvQrNrdWGdI/s6AtHikFPXOru5+4mJrDMnjc2mLIYXz+BMXW2dI\n9y8jqK/zkK7+7NT6Qro8O/p6XHsea30/SxytNKS9v5D8H/0kz3MISXURR5v4faG1t4fV+Xw8\no3RCUl3E0SbOuyLfo9ahrPX5jMs2wjKkr9dh2P47XcnstRQOyf9RlD9Ze5UV/MQNQ/raDD9e\njldSK6TjL/70/aCWEBbS5A+2+5+4YUhvw/t3Te+b7eFK6oXU/7NjCSEhrflHaxjS5njBz83z\nZ82QVvDsWETAPGcFU6FJhiGd2/nabuuGhBTLu5vwaVSHT2WGIT0PX+evtoTUoKXhHxpSl0eA\nhiG9D6+nrz6HbVpIHT6TdSQ4pKDvaozl8vfbpZ5/Q0pIXT6T9SSskD5Pz5qekP14OX/1+fpw\nLcO10YuHPpOx3zoz/kmEPdMRUvpFJJsIfADYb51V+EmElEtI6ReRbCI0pJBvWoWQn0SNvXeX\nj1CNkObnR5ObCDy3HvRdaxDwk6iz9+7ymKGhkMKeyQjpLCSkpW8opMNZbFMhhTyTEdLZ8k+C\nn5VOSyHFzGVXNzZGfjSLPwlC0mkrpBBdHoEvGb3Tiz8JQtLpL6Quj8CXTOx8wl7UU/CntZ6H\noqHlb0xK3bUU3nuv6eCAkHpwE1LUXqDoLmNN01VC6sFVSI72AquaghFSF36f+x3tBQhJfhGH\nm+jLw7teehi8nm5LcYRkqtyU5PwhS+f/LLSZKI72jsURkqHp+YssMF8h+ZmvFUdIhqaeoZUD\nztdegPNI2os43EQFk3uLx8GfPgDt9gLriSQEIdn4GXVTIT38e14MNgN8TYdtIQjJwu2omwnp\n+qObfA/SFm6jJUKycBp1U6+IO/996s3VgsGEFm6jKUIy8FvKfvRw6BzYTW++B2kLt9EUIRn4\nHXVXu5uroCb2RJ4HaQu30RQhGXgcdff7pqeD6//T+xht4TZaIiQLD6Pu/h9uwnpqYUWshdto\niZAsPOyA7v6+Cev2OM+vFm6jHUKycTvqJmZDj8/y7garuxvkBSHVMLWs8HTXm7fDJ3c3yA9C\nquJujjSxBuZuQu/uBvlBSFWMT5pGO3I0cN3dIEcIqZKAgzj1uM2e3xDSNEJyYvINHlXjVjC/\nIaRphORFylulRl2/4MqYI00iJB/S3io1ZgOX68y4QlbtJhGSD1MvDJeN2slTVZFXQ0bjCMmF\n8rOP65A4ONMjJJWs5+rgkDJ+B/148cDtIBIhaWQeMgWGlLOV29fFjn4DeaUjJI3c9aywy+dt\n5Wnul29ZR8hDSBLZcxyrTyOcTJGV7TyEJKEY4su7g5mthB6Xzf+2OyUlIyQJm3E4uZWY47Lx\n5AgpEyFp2BwZTW0lf+uElImQNGzm6hNbUVTAHCkPIakUXz0+bGB0K5KQWLXLQkiNmBvomuMy\nziPlIKRGzB56cVxWHSG1YX6nw3FZdYTUhqWjN47LKiOkNrA87RwhNYJpkG+E1AimQb4RUjOY\nBnlGSIAAIQEChOQeh3QtICTnii4y0KgMITlXcNmbhUAhQvKt5IlYTk0JEZJvBUOyeLHEeo4d\nCUmg4HBxENLsvVv4P/erOXYkpGzn4VImp3LHX2EhzcawUMqajh0JKdtpuCQ++y7lJ31WH/sk\n26C3AEv6PahVvdCWkHKdhknam2qHZCLb0z18TGDIxu/+Dv8/CanARRxuQub8rJx0HGN68HO5\npb9vuLrYKCEFIqRcOSGZDrXrXWfwTi4nJOZI8os43ITO8Zju6eo/4i4afalEacegOe8Vwaqd\n+iION6FzPVyCn+hvh7NhSLG7zpxVO84jqS/icBNKT4fF733ws+/v99rPkeJ3nennkdaEkGTC\nx9RvPqYHP7efkEQCUoRk7+aAznQ8H3adp6/2dgWvASHZq7ssnDKlwyJCslf7/MrsJ/chDSFV\nUH93QEhqhFRB/QkKIakRUhXVl8zq7xQ7Q0jrVH+n2BlCWqvqO8W+ENI6sUcSI6R1Yo4kRkir\nxKqdGiGtEiGpEdIqEZIaIa0TcyQxQlonVu3ECGmtOI8kRUiAACFVdNgpsGfoAiHlSg7h6p0b\n+L3v5hFSnoxJ++97kTDzbx8h5UlfRj6/H9fT+S2yCKllhJQl48Tmb0hPv/+BVhFSlv5DYvYW\nhpCy5LzU5jJHSnwDfgvM3kIRUp6MBO7XGTwOV7eFu0NIebKess/nkdw+7/Pa1mCElEsyiXA6\nEyGkYISEaYQUjJAwgzlSKELCDLezN3cICbOczt7cISRAgJAAAUICBAgJECAkQICQkIl1vR+E\nhCycaToiJGThtQ9HhIQcvBrvhJCQg5BOCAk5COmEkJCFOdIRISELq3ZHhIRMnEf6QUiAACEB\nAoSkwNHN6hFSPubbICQByxVg9n1OEVI2w3OS7PvcIqRsliFZbQixCCmbXUi8HscvQspntp8I\nColZVBWElM9s5hIQErOoSghJwWovsLzvYxZVCSG1ZHF/wyyqFkJqy8K+j5BqIaSuEFIthNSk\nyR0Tc6RKCKlBM1MlVu0qIaQGze52OI9UBSG1h4mQQ4TUHkJyiJDaQ0gOEVKDWJrzh5AaxNKc\nP4TUJJbmvCEkQKBKSMPSVRASGkNIgIBhSMOtEpsAKjEM6b8NIaFXlod2Xy/D9vNwDRzaoTO2\nc6S/w/B3T0joj/Fiw+d2ePkiJHTHfNXuz7D5R0jojf3y98fz+EpD8EoE4E+N80iv7JHQG14i\nBAjUCGn5yI2Q0BhCAgQICRAgJECAkAABQgIEWP4GBAgJECAkQICQAAFCAgQICRAgJECAkAAB\nQgIECCka77uNR4QUyfsnQZB5HYQUyfdnE3nPvF+EFMf5p+X5zrxnhBTHd0i+b13XCCmO76Hq\n+9Z1jZAiuT54IqRqCCmS7+m868y7RkjRPC8w+868Z4TUGc+Z94yQAAFCAgQICRAgJECAkAAB\nQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIEnIYENCZhlOvDMWF9\nu81/Tr3fwd62R0g+t9f9Hexte4Tkc3vd38HetkdIPrfX/R3sbXuE5HN73d/B3rZHSD631/0d\n7G17hORze93fwd62R0g+t9f9Hexte4Tkc3vd38HetkdIPrfX/R3sbXuE5HN73d/B3rZHSD63\n1/0d7G17rYYEuEJIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAh\nAQKEBAg0G9L787B5+zLdot3P6m3T8Z3bmz94X6/D8PpRdhuthvR2+NCAjeFg+0j5jII028Od\ne7ba3N70zu3tH7zNYXtlS2o0pI/h9evnafTVbosbs7H237D5+Nnef0bbM71ze/sH7+1nS2/D\nS9GNNBrSy/F22z3878PWbGNvw7/vP/8Of4y2Z3rn9vYP3mb4Kr+5RkM6sXv4hze7jb0Mn/uf\n5+2yT6FXLMM6XA4AAAKaSURBVO/c1VZtNzlsyl590Wsv7GvYWm3qw/CBH6x3t5Z37sLwwfvx\nNrwXvf6mQ3o/HANZ6Tck640dmD54f4fvvW5RLYf0uTE7+PlBSEq2D977y6bwlLPhkL42pscG\nhKRk/eDt969lj+0aC+n6M6e3BidarrdnuMpkuz3zje1tHrxbX2VXG5oN6fN5+2m5PcOxdly1\n+7Rbtdtbh2Tz4N0pexcbC+nin+2azw+zsfbnMA3/V3p6fMM0JOMH73ge6bPsS0UaDenTviO7\nsWb/ygbbkKwfvMMrG75emCONeB2G66MuE3Ybez7cNdPRZvmTNH/wNgY/z0ZDGroO6evw6m+r\nrR1Y/iTtH7zvn+dz2fOxrYYE+EJIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQE\nCBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQ\nEiBASIAAIQEChAQIEBIgQEhte+cB9IHHoWkfph/+iWk8Di372BCSEzwODXsftoTkBI9Da7bD\nf99//je8fj94b6afRo4ZPA6t+Rw2339uNl/fR3Z7QvKCx6E578Of/Z/h7/E/CMkJHof2bIf3\n4eX0NSE5wePQns9hGD5PXxOSEzwODXob3s5fEpITPA7tYY/kEI9De16+50jb09eE5ASPQ3P+\nfh/Y/Rnej/9BSE7wOLTma3M4j3Q6uCMkJ3gcWvN6emXD8eCOkJzgcQAECAkQICRAgJAAAUIC\nBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQI\nCRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQOD/OHscYq5+vAUAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(4)\n",
    "x1=rnorm(200, mean= 0)\n",
    "x2=rnorm(200, mean= 1)\n",
    "x1[101:200]=rnorm(100, mean = 0)\n",
    "x2[101:200]=rnorm(100, mean = -2)\n",
    "df=data.frame(cbind(x1,x2))\n",
    "x1=df$x1\n",
    "x2=df$x2\n",
    "plot(x1, x2, col=y+2)\n",
    "x2 = rep(-1/2, 200)  ##(mean12+mean21)/2\n",
    "lines(x1, x2, col='blue', type='l')\n",
    "#curve(nd(x1, x2, 1, 1)-nd(x1, x2, -2, -2), -5:5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 2.3 Derive equation (2.24)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median distance corresponds to the volume where half of the particles is inside the volume and half is outside. Probability that particle is inside volume fraction v equils probability that observation is outside volume v, $(1-v)^{N} = 1-(1-v)^N ->v =1-0.5^{1/N}$ then $d(N, p)=(1-0.5^{1/N})^{1/p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_i$ are projections of unit vector. Initial vector point in random direction randomly. Therefore it is clear that for projections in any direction we will be uniformly distributed and have mean zero. For mean distance vector $\\sqrt{\\sum_{i=0}^n\\sum_{j=0}^px_{ij}^2/N}=p$.\n",
    "Because the vectors orientation is uniformly distributed in all directions, if sum in all directions equals p, for one direction we divide it by p:\n",
    "$\\sigma =\\sqrt{\\sum_{i=0}^n{x_{ij}^2}/N}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Derive equation (2.27). The last line makes use of (3.8) through a\n",
    "conditioning argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added $f(x_0)=x_0^T\\beta$ to the equation as\n",
    "$E_{y_0|x_0}E_{\\tau}((y_0-f(x_0))-((\\hat{y_0}-f(x_0))^2=E_{y_0|x_0}E_{\\tau}(y_0-f(x_0))^2-2E_{y_0|x_0}E_{\\tau}((y_0-f(x_0))(\\hat{y_0}-f(x_0)))+E_{y_0|x_0}E_{\\tau}(\\hat{y_0}-f(x_0))^2$ ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is $Var(y_0|x_0)$, the last term is $Var(\\hat{y_0})+Bias(\\hat{y_0})$ from Eq.(2.25) (to derive(2.25) we need to add and sutract $E_{\\tau}(\\hat{y_0})$ from initial equation), term in the middle goes to 0. We have\n",
    "\n",
    "$Var(y_0|x_0)+Var(\\hat{y_0})+Bias(\\hat{y_0})$ = \n",
    "\n",
    "Using $\\hat{y_0}=x_0^T\\beta+\\sum_{i=1}^NX^T(X^TX)^{-1}x_0\\epsilon_i$ and there is no bias we have result\n",
    "\n",
    "$\\sigma^2 + E_{\\tau}x_0^T(X^TX)^{-1}x_0\\sigma^2+0^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Derive equation (2.28), making use of the cyclic property of the trace\n",
    "operator [trace(AB) = trace(BA)], and its linearity (which allows us\n",
    "to interchange the order of trace and expectation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking trace and using cycling property\n",
    "\n",
    "$\\sigma^2 + E_{x_0}tr(x_0^TCov(X^{-1})x_0)\\sigma^2/N = \\sigma^2 + E_{x_0}tr(Cov(X^{-1})x_0^Tx_0)\\sigma^2/N = \\sigma^2 + tr(Cov(X^{-1})E_{x_0}(x_0^Tx_0))\\sigma^2/N = Tr(Cov(X^{-1})Cov(x_0))\\sigma^2/N + \\sigma^2$\n",
    "\n",
    "$Tr(Cov(X^{-1})Cov(x_0))=tr(I_p) = p$ we get result\n",
    "\n",
    "$=\\sigma^2p/N+\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a regression problem with inputs xi and outputs yi, and a\n",
    "parameterized model $f_θ(x)$ to be fit by least squares. Show that if there are\n",
    "observations with tied or identical values of x, then the fit can be obtained\n",
    "from a reduced weighted least squares problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimize RSS(f)=$\\sum_{i=1}^n(y_i-f(x_i))^2$ if we have j identical x_i mean $\\hat{y}_i=\\sum_j{y_{ji}}/n_j$. Substituting to RSS\n",
    "\n",
    "RSS(f) = $\\sum_{i=1}^{n'}n_i(\\hat{y}_i-f(x_i))^2$, where $n'$ is number of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a sample of N pairs xi, yi drawn i.i.d. from the\n",
    "distribution characterized as follows:\n",
    "xi ∼ h(x), the design density\n",
    "yi = f(xi) + εi, f is the regression function\n",
    "εi ∼ (0, σ2) (mean zero, variance σ2)\n",
    "We construct an estimator for f linear in the yi,\n",
    "$\\hat{f}(x_0) = \\sum_{i=1}^Nℓ_i(x_0;X)yi,$\n",
    "where the weights ℓi(x0;X) do not depend on the yi, but do depend on the\n",
    "entire training sequence of xi, denoted here by X.\n",
    "\n",
    "(a) Show that linear regression and k-nearest-neighbor regression are members\n",
    "of this class of estimators. Describe explicitly the weights ℓi(x0;X)\n",
    "in each of these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression $f(x)=x^T\\beta$ then the estimator $\\hat{f}(x_0)=\\sum_{i=1}^N{(XX^TX)_iy_i}$\n",
    "\n",
    "For K nearest neighbours $\\hat{f}(x_0) = 1/k\\sum_i I_{xinN_k(x_0)}y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Decompose the conditional mean-squared error\n",
    "$E_{Y|X} (f(x_0) − \\hat{f}(x_0))^2$\n",
    "into a conditional squared bias and a conditional variance component.\n",
    "Like X, Y represents the entire training sequence of yi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_{Y|X} (f(x_0) − \\hat{f}(x_0))^2 = E_{Y|X} ((f(x_0)- E_{Y|X} f(x_0)) + (E_{Y|X} f(x_0) − \\hat{f}(x_0)))^2= E_{Y|X}(f(x_0)- E_{Y|X} f(x_0))^2 + (E_{Y|X} f(x_0) − \\hat{f}(x_0))^2 = Var(f(X))+Bias(f(X))^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Decompose the (unconditional) mean-squared error $E_{Y,X} (f(x_0) − \\hat{f}(x_0))^2$ into a squared bias and a variance component.\n",
    "\n",
    "$E_{Y,X} (f(x_0) − \\hat{f}(x_0))^2 = E_{Y,X} ((f(x_0)- E_{Y,X} f(x_0)) + (E_{Y,X} f(x_0) − \\hat{f}(x_0)))^2= E_{Y,X}(f(x_0)- E_{Y,X} f(x_0))^2 + (E_{Y,X} f(x_0) − \\hat{f}(x_0))^2 = Var(f(x_0))+Bias(\\hat{f}(x_0))^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Establish a relationship between the squared biases and variances in\n",
    "the above two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(X) = E(f(x)|X), so $Var(f(X))+Bias(f(X))^2 = E(Var(f(x_0))+Bias(\\hat{f}(x_0))^2|x_0=X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the classification performance of linear regression and k–\n",
    "nearest neighbor classification on the zipcode data. In particular, consider\n",
    "only the 2’s and 3’s, and k = 1, 3, 5, 7 and 15. Show both the training and\n",
    "test error for each choice. The zipcode data are available from the book\n",
    "website www-stat.stanford.edu/ElemStatLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>V1</th><th scope=col>V3</th><th scope=col>V5</th><th scope=col>V7</th><th scope=col>V15</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>5</th><td>3     </td><td>-1    </td><td>-1.00 </td><td>-0.928</td><td>-1    </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>3     </td><td>-1    </td><td>-0.83 </td><td> 1.000</td><td>-1    </td></tr>\n",
       "\t<tr><th scope=row>27</th><td>3     </td><td>-1    </td><td>-1.00 </td><td>-0.104</td><td>-1    </td></tr>\n",
       "\t<tr><th scope=row>31</th><td>3     </td><td>-1    </td><td>-1.00 </td><td>-1.000</td><td>-1    </td></tr>\n",
       "\t<tr><th scope=row>36</th><td>3     </td><td>-1    </td><td>-1.00 </td><td> 0.492</td><td>-1    </td></tr>\n",
       "\t<tr><th scope=row>42</th><td>2     </td><td>-1    </td><td>-1.00 </td><td>-0.798</td><td>-1    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & V1 & V3 & V5 & V7 & V15\\\\\n",
       "\\hline\n",
       "\t5 & 3      & -1     & -1.00  & -0.928 & -1    \\\\\n",
       "\t7 & 3      & -1     & -0.83  &  1.000 & -1    \\\\\n",
       "\t27 & 3      & -1     & -1.00  & -0.104 & -1    \\\\\n",
       "\t31 & 3      & -1     & -1.00  & -1.000 & -1    \\\\\n",
       "\t36 & 3      & -1     & -1.00  &  0.492 & -1    \\\\\n",
       "\t42 & 2      & -1     & -1.00  & -0.798 & -1    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | V1 | V3 | V5 | V7 | V15 |\n",
       "|---|---|---|---|---|---|\n",
       "| 5 | 3      | -1     | -1.00  | -0.928 | -1     |\n",
       "| 7 | 3      | -1     | -0.83  |  1.000 | -1     |\n",
       "| 27 | 3      | -1     | -1.00  | -0.104 | -1     |\n",
       "| 31 | 3      | -1     | -1.00  | -1.000 | -1     |\n",
       "| 36 | 3      | -1     | -1.00  |  0.492 | -1     |\n",
       "| 42 | 2      | -1     | -1.00  | -0.798 | -1     |\n",
       "\n"
      ],
      "text/plain": [
       "   V1 V3 V5    V7     V15\n",
       "5  3  -1 -1.00 -0.928 -1 \n",
       "7  3  -1 -0.83  1.000 -1 \n",
       "27 3  -1 -1.00 -0.104 -1 \n",
       "31 3  -1 -1.00 -1.000 -1 \n",
       "36 3  -1 -1.00  0.492 -1 \n",
       "42 2  -1 -1.00 -0.798 -1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train <- read.table(\"zip_train\")\n",
    "test <- read.table(\"zip_test\")\n",
    "## Filtering to 2’s and 3’s\n",
    "train <- train[train[,1] %in% c(2, 3),]\n",
    "test <- test[test[,1] %in% c(2, 3),]\n",
    "pixels <- c(\"V1\", \"V3\", \"V5\", \"V7\", \"V15\")\n",
    "train <- train[,pixels]\n",
    "test <- test[,pixels]\n",
    "head(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.395604395604396"
      ],
      "text/latex": [
       "0.395604395604396"
      ],
      "text/markdown": [
       "0.395604395604396"
      ],
      "text/plain": [
       "[1] 0.3956044"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear regression.\n",
    "linear.mod <- lm(train[,1]~., data=train[,-1])\n",
    "predict <- predict(linear.mod, test[,2:5])\n",
    "error.lin <- mean(pred.lin!=test[,1])\n",
    "error.lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.42032967032967"
      ],
      "text/latex": [
       "0.42032967032967"
      ],
      "text/markdown": [
       "0.42032967032967"
      ],
      "text/plain": [
       "[1] 0.4203297"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "require(class)\n",
    "pred.knn <- knn(train[,2:5], test[,2:5], train[,1], k=6)\n",
    "error.knn <- mean(pred.knn!=test[,1])\n",
    "error.knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn gives a bit better error for k=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prove that\n",
    "$E[R_{tr}(\\hat{\\beta})] ≤ E[R_{te}(\\hat{\\beta})]$, $R_{tr} = 1/N\\sum_{i=1}^N(y_i-x_i^T\\hat{\\beta})^2$, $R_{te} = 1/M\\sum_{i=1}^M(y_i'-x_i'^T\\hat{\\beta})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This property is much based on fact that $\\hat{\\beta}$ was obtained minimizing $R_{tr}$. \n",
    "$1/N\\sum_{i=1}^N(y_i-x_i^T\\hat{\\beta})^2 <= 1/N\\sum_{i=1}^N(y_i-x_i^T\\beta)^2$, for any $/beta$\n",
    "\n",
    "Because data from the same distribution we can get rid from how many observations in the sample\n",
    "\n",
    "$E(1/N\\sum_{i=1}^N(y_i-x_i^T\\hat{\\beta})^2) <= E(y_i-x_i^T\\beta)^2$ (1), for any beta\n",
    "\n",
    "Now we can change beta on right side to transform to another $\\beta'$ by condition $y-x\\beta =  y'-x'\\beta'$\n",
    "inequility  will continue to hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other approach is if we do minimization of $R_{te}$ on test data set and obtain $\\hat{\\beta}'$, then $1/N\\sum_{i=1}^N(y_i-x_i^T\\hat{\\beta})^2 <= 1/N\\sum_{i=1}^N(y_i-x_i^T\\hat{\\beta'})^2$, when $1/M\\sum_{i=1}^M(y_i'-x_i'^T\\hat{\\beta'})^2<=1/M\\sum_{i=1}^M(y_i'-x_i'^T\\hat{\\beta})^2$, from this we come to inquility we need to prove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
